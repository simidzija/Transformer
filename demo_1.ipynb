{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: copying a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we train a decoder-only transformer to simply repeat back an integer sequence. We expect the transformer, which contains a masked multihead attention layer, to be able to accomplish this task because each position of the sequence is able to attend to itself (and all previous positions), so in particular it should be able to copy itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer import layers\n",
    "\n",
    "# set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device is {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is a standard decoder-only transformer, consisting of the following layers:\n",
    "- embedding and positional encoding\n",
    "- decoder stack\n",
    "- linear (de-embedding) layer\n",
    "- softmax\n",
    "\n",
    "Each decoder in the decoder stack consists of a masked multihead attention layer followed by a position-wise fully connected two-layer feed forward neural network. We will restrict to input sequences of length 10, containing integers from 0 to 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "n_min = 0\n",
    "n_max = 99\n",
    "\n",
    "model = layers.Transformer(\n",
    "    vocab=100,\n",
    "    n_pe=1000,\n",
    "    d_model=32,\n",
    "    num_heads=4,\n",
    "    num_stacks=2,\n",
    "    d_ff=64,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the randomly initialized model performs on the sequence copying task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy: 0.0%\n"
     ]
    }
   ],
   "source": [
    "test_data = torch.randint(\n",
    "    low=n_min,\n",
    "    high=n_max,\n",
    "    size=(100, seq_len) # 100 sequences\n",
    ")\n",
    "\n",
    "def test(data, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = data\n",
    "        output = model.greedy_output(input)\n",
    "\n",
    "        n_total = len(input)\n",
    "\n",
    "        # number of output sequences that match their input sequence\n",
    "        n_correct = (input == output).all(-1).sum().item() \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return n_correct / n_total\n",
    "\n",
    "accuracy = test(test_data, model)\n",
    "print(f'Initial accuracy: {accuracy:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the untrained transformer can't copy the sequence. \n",
    "\n",
    "Now let's train the model. We will use cross entropy loss and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracies:\n",
      "  Step   0: 0.0%\n",
      "  Step  20: 0.0%\n",
      "  Step  40: 0.0%\n",
      "  Step  60: 24.0%\n",
      "  Step  80: 74.0%\n",
      "  Step 100: 100.0%\n",
      "  Step 120: 100.0%\n",
      "  Step 140: 100.0%\n",
      "  Step 160: 100.0%\n",
      "  Step 180: 100.0%\n",
      "  Step 199: 100.0%\n"
     ]
    }
   ],
   "source": [
    "cost_fn = layers.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, steps, batch_size, seq_len, grad_norm):\n",
    "    # steps between tests\n",
    "    test_interval = steps // 10 \n",
    "    print(f'Test set accuracies:')\n",
    "\n",
    "    for step in range(steps):\n",
    "        # randomly generate input sequences\n",
    "        input = torch.randint(n_min, n_max, (batch_size, seq_len))\n",
    "\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "\n",
    "        # compute loss. transpose puts class dim in correct position for CE loss\n",
    "        loss = cost_fn(output.transpose(1,2), input)\n",
    "\n",
    "        # backward pass\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm)\n",
    "\n",
    "        # optimizer step\n",
    "        optim.step()\n",
    "\n",
    "        # test\n",
    "        if step % test_interval == 0 or step == steps - 1:\n",
    "            accuracy = test(test_data, model)\n",
    "            print(f'  Step {step:3d}: {accuracy:.1%}')\n",
    "\n",
    "train(model, 200, 64, 10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model is able to perfectly copy the test sequences. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:  [56, 4, 43, 81, 65, 9, 71, 11, 21, 79]\n",
      "Output sequence: [56, 4, 43, 81, 65, 9, 71, 11, 21, 79]\n"
     ]
    }
   ],
   "source": [
    "input = torch.randint(n_min, n_max, (10,))\n",
    "output = model.greedy_output(input)\n",
    "\n",
    "print(f'Input sequence:  {input.tolist()}')\n",
    "print(f'Output sequence: {output.tolist()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
