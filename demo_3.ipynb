{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 3: Beatles lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we will train a decoder-only transformer to generate lyrics in the style of The Beatles.\n",
    "We will do this by training it on a corpus consisting of Beatles lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformer import layers\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA is available. Using GPU.')\n",
    "else: \n",
    "    device = torch.device('cpu')\n",
    "    print('CUDA not available. Using CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by loading the data and tokenizing it. \n",
    "We will tokenize by splitting the corpus at word boundaries. \n",
    "\n",
    "First we create a `Vocab` class which initializes a vocabulary using the specified corpus, and which contains three methods:\n",
    "- `tokens`: maps a string to a list of integer tokens\n",
    "- `words`: maps a list of integer tokens to the corresponding string\n",
    "- `tokenize_from_file`: tokenizes a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join('data/beatles.txt')\n",
    "\n",
    "# special tokens\n",
    "sos = -1   # start of sequence\n",
    "eos = -2   # end of sequence\n",
    "unk = -3   # unknown subword\n",
    "pad = -100 # padding\n",
    "\n",
    "class Vocab():\n",
    "    def __init__(self, filepath, pattern=r'(?<=\\w)(?=\\W)|(?<=\\W)(?=\\w)'):\n",
    "        self.pattern = pattern\n",
    "        self.words_to_tokens = {}\n",
    "        self.tokens_to_words = {}\n",
    "        with open(filepath, mode='r') as file:\n",
    "            token = 0 # next token to be assigned\n",
    "            for line in file:\n",
    "                words = re.split(pattern, line, flags=re.IGNORECASE)\n",
    "                for word in words:\n",
    "                    if word not in self.words_to_tokens:\n",
    "                        self.words_to_tokens[word] = token\n",
    "                        self.tokens_to_words[token] = word\n",
    "                        token += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.words_to_tokens)\n",
    "\n",
    "    def tokens(self, str: str) -> list[int]:\n",
    "        word_lst = re.split(self.pattern, str, flags=re.IGNORECASE)\n",
    "        tok_lst = []\n",
    "        for word in word_lst:\n",
    "            if word in self.words_to_tokens:\n",
    "                tok_lst.append(self.words_to_tokens[word])\n",
    "            else:\n",
    "                tok_lst.append(unk)\n",
    "\n",
    "        return tok_lst\n",
    "\n",
    "    def words(self, tokens: list[int]) -> str:\n",
    "        word_lst = []\n",
    "        for token in tokens:\n",
    "            if token in self.tokens_to_words:\n",
    "                word_lst.append(self.tokens_to_words[token])\n",
    "            else:\n",
    "                raise ValueError(f'{token} not a valid token')\n",
    "            \n",
    "        return ''.join(word_lst)\n",
    "    \n",
    "    def tokenize_from_file(self, filepath: str) -> list[int]:\n",
    "        tokens = []\n",
    "        with open(filepath, mode='r') as file:\n",
    "            for line in file:\n",
    "                tokens.extend(self.tokens(line))\n",
    "\n",
    "        return tokens\n",
    "\n",
    "vocab = Vocab(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the vocab to tokenize the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = torch.LongTensor(vocab.tokenize_from_file(filepath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a class `TokenizedDataset` which sections off a corpus into fixed size training examples. \n",
    "Each training example is a pair (input, target) where input is the same token sequence as target, but right-shifted by one position to accomodate the start-of-sequence token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenizedDataset(Dataset):\n",
    "    def __init__(self, corpus: torch.LongTensor, context_size: int):\n",
    "        assert context_size <= len(corpus)\n",
    "        super().__init__()\n",
    "\n",
    "        self.corpus = corpus\n",
    "        self.context_size = context_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.corpus) - self.context_size + 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        assert idx >= 0 and idx < len(self), f'index {idx} out of range'\n",
    "\n",
    "        context = self.corpus[idx:idx + self.context_size]\n",
    "        input = torch.cat([torch.IntTensor([sos]), context])\n",
    "        target = torch.cat([context, torch.IntTensor([pad])])\n",
    "        \n",
    "        return (input, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our corpus so that 90% goes into the training set and 10% into the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_size = 32\n",
    "batch_size = 32\n",
    "\n",
    "train_set = TokenizedDataset(corpus[:round(0.9 * len(corpus))], context_size)\n",
    "test_set = TokenizedDataset(corpus[round(0.9 * len(corpus)):], context_size)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data, let's construct the decoder-only transformer. It will consist of the following layers:\n",
    "- embedding and positional encoding\n",
    "- decoder stack\n",
    "- linear (de-embedding) layer\n",
    "- softmax\n",
    "\n",
    "Each decoder in the decoder stack consists of a masked multihead attention layer followed by a position-wise fully connected two-layer feed forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = layers.Transformer(\n",
    "    vocab=len(vocab),\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    num_stacks=4,\n",
    "    d_ff=256,\n",
    "    dropout=0.0\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the testing and training loops. We will use a cross entropy loss function and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost_fn = layers.CrossEntropyLoss(ignore_index=pad, swap_dims=True)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for input, target in test_loader:\n",
    "            output = model(input)\n",
    "            total_loss += cost_fn(output, target)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    num_tokens = len(test_loader.dataset.corpus)\n",
    "\n",
    "    # return average loss per token\n",
    "    return total_loss / num_tokens \n",
    "\n",
    "def train(model):\n",
    "    print_period = len(train_loader) // 100\n",
    "\n",
    "    for batch, (input, target) in enumerate(train_loader):\n",
    "        # output\n",
    "        output = model(input)\n",
    "\n",
    "        # loss\n",
    "        loss = cost_fn(output, target)\n",
    "\n",
    "        # backprop\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # step\n",
    "        optim.step()\n",
    "\n",
    "        # print progress\n",
    "        if batch % print_period == 0 or batch == len(train_loader) - 1:\n",
    "            test_loss = test(model)\n",
    "            print(f'Batch {batch:3d}. Test loss = {test_loss:5.3}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
