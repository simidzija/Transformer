{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo 3: Beatles lyrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo we will train a decoder-only transformer to generate lyrics in the style of The Beatles.\n",
    "We will do this by training it on a corpus consisting of Beatles lyrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# set to true if working in google colab\n",
    "colab = False\n",
    "\n",
    "if colab:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    root_dir = '/content/drive/MyDrive/Projects/Transformer'\n",
    "    if root_dir not in sys.path:\n",
    "        sys.path.append(root_dir)\n",
    "else:\n",
    "    root_dir = os.getcwd()\n",
    "\n",
    "# custom imports\n",
    "from transformer import layers\n",
    "from transformer.layers import Transformer\n",
    "from transformer.nlp import Vocab, TokenizedDataset, InferenceSampler\n",
    "\n",
    "# set device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print('CUDA is available. Using GPU.')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('CUDA not available. Using CPU.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the custom Vocab class to load the text corpus and initialize the vocabulary according to this corpus. We will utilize a subword tokenization scheme in which we split the corpus at boundaries between word and non-word characters. The Vocab class provides useful methods for converting strings to tokens and vice versa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = os.path.join(root_dir, 'data/beatles.txt')\n",
    "\n",
    "# special tokens\n",
    "sos = -1   # start of sequence\n",
    "unk = -3   # unknown subword\n",
    "pad = -100 # padding\n",
    "\n",
    "# pattern matching boundary between word and non-word characters\n",
    "pattern = r'(?<=\\w)(?=\\W)|(?<=\\W)(?=\\w)'\n",
    "\n",
    "vocab = Vocab(filepath, sos, unk, pad, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the vocab to tokenize the corpus, using the method `tokenize_from_file`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus: tensor([   0,    1,    2,  ..., 2978,    1,    5])\n"
     ]
    }
   ],
   "source": [
    "corpus = vocab.tokenize_from_file(filepath)  # list\n",
    "corpus = torch.tensor(corpus, dtype=torch.long, device=device)  # tensor\n",
    "\n",
    "print(f'Corpus: {corpus}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split our corpus so that 90% goes into the training set and 10% into the test set. We use the custom class TokenizedDataset to define the dataset. This class creates training examples consisting of (input, target) pairs, where input is the same token sequence as target but right shifted by one position to allow space for the start-of-sequence (sos) token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_window = 32\n",
    "batch_size = 32\n",
    "\n",
    "train_corpus = corpus[:round(0.9 * len(corpus))]\n",
    "test_corpus = corpus[round(0.9 * len(corpus)):]\n",
    "\n",
    "train_set = TokenizedDataset(train_corpus, context_window, sos, pad, device)\n",
    "test_set = TokenizedDataset(test_corpus, context_window, sos, pad, device)\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the decoder-only transformer. It consists of the following layers:\n",
    "- embedding and positional encoding\n",
    "- decoder stack\n",
    "- linear (de-embedding) layer\n",
    "- softmax\n",
    "\n",
    "Each decoder in the decoder stack consists of a masked multihead attention layer followed by a position-wise fully connected two-layer feed forward neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer(\n",
    "    vocab=len(vocab),\n",
    "    d_model=8,\n",
    "    num_heads=2,\n",
    "    num_stacks=1,\n",
    "    d_ff=16,\n",
    "    dropout=0.0\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the untrained model performs. To do this we first define an InferenceSampler object using the model and vocab. The custom InferenceSampler class provides methods for autoregressively generating outputs given a prompt, according to different sampling strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = InferenceSampler(model, vocab, context_window, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sampler allows for three sampling methods: greedy sampling, top_p sampling, and beam search. For example, greedy sampling simply outputs the most next token, given the previous tokens. Let's consider the input prompt 'You and me baby'. Greedy sampling gives the following output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "armdoingshelterintosuddenlycrackermibelowshelterintorevolutionrevolutionrevolutionrevolutionMorningintointoarrivinglitdesireSavingtellingrevolutionsuddenlyarriving\n"
     ]
    }
   ],
   "source": [
    "prompt = 'You and me baby'\n",
    "output = sampler.greedy(prompt)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "armdoingshelterintosuddenlycrackermibelowshelterintorevolutionrevolutionrevolutionrevolutionMorningintointoarrivinglitdesireSavingtellingrevolutionsuddenlyarriving\n"
     ]
    }
   ],
   "source": [
    "output = sampler.top_p(prompt, temp=1, p=0.0)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WantdiAsknaturalcrackerLorettadriverboysdreamsnaturalSeeringingrockbagintosucceedopportunityopportunitylalafingersliketellingsuddenlyvontStarIconBronzeCertIconLangIcon\n"
     ]
    }
   ],
   "source": [
    "output = sampler.beam_search(prompt, 10)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly the untrained transformer's output doesn't make any sense. \n",
    "\n",
    "Now let's define the testing and training loops. We will use a cross entropy loss function and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 1130/2168. Test loss = 3.32\n",
      "Ran out of patience.\n"
     ]
    }
   ],
   "source": [
    "cost_fn = layers.CrossEntropyLoss(ignore_index=pad, swap_dims=True)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_loss = 0\n",
    "        for input, target in test_loader:\n",
    "            output = model(input)\n",
    "            total_loss += cost_fn(output, target)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # return average loss\n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "def train(model, epochs, test_period, starting_patience):\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    # early stopping variables\n",
    "    patience = starting_patience\n",
    "    min_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for batch, (input, target) in enumerate(train_loader):\n",
    "\n",
    "            # output\n",
    "            output = model(input)\n",
    "\n",
    "            # loss\n",
    "            loss = cost_fn(output, target)\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "            # backprop\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            # step\n",
    "            optim.step()\n",
    "\n",
    "            # test\n",
    "            if batch % test_period == 0 or batch == len(train_loader) - 1:\n",
    "                test_loss = test(model)\n",
    "                test_losses.append(test_loss)\n",
    "\n",
    "                # print progress\n",
    "                clear_output(wait=True)\n",
    "                print(f'Epoch {epoch}, batch {batch}/{len(train_loader)}. ' \n",
    "                      f'Test loss = {test_loss:.3}')\n",
    "\n",
    "                # abort if no more patience (after epoch 0)\n",
    "                if epoch > -1:\n",
    "                    if test_loss < 0.99 * min_loss:\n",
    "                        min_loss = test_loss\n",
    "                        patience = starting_patience\n",
    "                    else:\n",
    "                        patience -= 1\n",
    "                        if patience < 0:\n",
    "                            # save checkpoint\n",
    "                            path = os.path.join(\n",
    "                                f'checkpoints/e{epoch}b{batch}.pt')\n",
    "                            torch.save(model.state_dict(), path)\n",
    "\n",
    "                            print('Ran out of patience.')\n",
    "                            return train_losses, test_losses\n",
    "                        \n",
    "        path = os.path.join(f'checkpoints/e{epoch}b{batch}.pt')\n",
    "        torch.save(model.state_dict(), path)\n",
    "\n",
    "    return train_losses, test_losses\n",
    "    \n",
    "losses = train(model, epochs=5, test_period=10, starting_patience=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
