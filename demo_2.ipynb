{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo: reversing a sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we train a decoder-only transformer to reverse an integer sequence.\n",
    "\n",
    "Our tranformer will be built using a masked multihead attention layer which enables each position in an input sequence to attend to itself and all previous positions, but not to latter positions.\n",
    "Therefore we expect that the transformer should be able to correctly output the second half of reversed sequence, but not the first half."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformer import layers\n",
    "\n",
    "# set device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device is {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is a standard decoder-only transformer, consisting of the following layers:\n",
    "- embedding and positional encoding\n",
    "- decoder stack\n",
    "- linear (de-embedding) layer\n",
    "- softmax\n",
    "\n",
    "Each decoder in the decoder stack consists of a masked multihead attention layer followed by a position-wise fully connected two-layer feed forward neural network. We will restrict to input sequences of length 10, containing integers from 0 to 99."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "n_min = 0\n",
    "n_max = 99\n",
    "\n",
    "model = layers.Transformer(\n",
    "    vocab=100,\n",
    "    n_pe=1000,\n",
    "    d_model=32,\n",
    "    num_heads=4,\n",
    "    num_stacks=2,\n",
    "    d_ff=64,\n",
    "    dropout=0.1\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the randomly initialized model performs on the sequence reversal task. Here is one example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:  [20, 7, 68, 34, 55, 93, 85, 40, 74, 7]\n",
      "Output sequence: [32, 99, 6, 23, 15, 22, 98, 3, 34, 22]\n"
     ]
    }
   ],
   "source": [
    "input = torch.randint(n_min, n_max, (10,))\n",
    "output = model.greedy_output(input)\n",
    "\n",
    "print(f'Input sequence:  {input.tolist()}')\n",
    "print(f'Output sequence: {output.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has no idea what it's doing. \n",
    "Let's define a test set containing 100 examples, each of length 10, and test the model's performance on this set.\n",
    "In particular we will seperately track the model's ability to generate the first 5 target integers and the second 5 target integers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial accuracy:\n",
      "  1st half: 0.0%\n",
      "  2nd half: 0.0%\n"
     ]
    }
   ],
   "source": [
    "test_data = torch.randint(\n",
    "    low=n_min,\n",
    "    high=n_max,\n",
    "    size=(100, seq_len) # 100 sequences\n",
    ")\n",
    "\n",
    "def test(data, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        input = data\n",
    "        output = model.greedy_output(input)\n",
    "        reversed_input = torch.flip(input, dims=[1])\n",
    "\n",
    "        # split into halves\n",
    "        rev_in_half_1, rev_in_half_2 = reversed_input.chunk(2, dim=1)\n",
    "        out_half_1, out_half_2 = output.chunk(2, dim=1)\n",
    "\n",
    "        # count correct predictions\n",
    "        n_total = len(input)\n",
    "        n_correct_1 = (out_half_1 == rev_in_half_1).all(-1).sum().item() \n",
    "        n_correct_2 = (out_half_2 == rev_in_half_2).all(-1).sum().item() \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return n_correct_1 / n_total, n_correct_2 / n_total\n",
    "\n",
    "accuracy_1, accuracy_2 = test(test_data, model)\n",
    "print(f'Initial accuracy:\\n'\n",
    "      f'  1st half: {accuracy_1:.1%}\\n'\n",
    "      f'  2nd half: {accuracy_2:.1%}'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the untrained transformer can't reverse either half of the sequence. \n",
    "\n",
    "Now let's train the model. We will use cross entropy loss and an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set accuracies (1st half, 2nd half):\n",
      "  Step    0: (0.0%,   0.0%)\n",
      "  Step  200: (0.0%,   0.0%)\n",
      "  Step  400: (0.0%,   0.0%)\n",
      "  Step  600: (0.0%,   0.0%)\n",
      "  Step  800: (0.0%,   3.0%)\n",
      "  Step 1000: (0.0%,  37.0%)\n",
      "  Step 1200: (0.0%,  91.0%)\n",
      "  Step 1400: (0.0%,  99.0%)\n",
      "  Step 1600: (0.0%, 100.0%)\n",
      "  Step 1800: (0.0%, 100.0%)\n",
      "  Step 1999: (0.0%, 100.0%)\n"
     ]
    }
   ],
   "source": [
    "cost_fn = layers.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(model, steps, batch_size, seq_len, grad_norm):\n",
    "    # steps between tests\n",
    "    test_interval = steps // 10 \n",
    "    print(f'Test set accuracies (1st half, 2nd half):')\n",
    "\n",
    "    for step in range(steps):\n",
    "        # randomly generate input sequences\n",
    "        input = torch.randint(n_min, n_max, (batch_size, seq_len))\n",
    "\n",
    "        # compute output and reversed input\n",
    "        output = model(input)\n",
    "        reversed_input = torch.flip(input, dims=[1])\n",
    "\n",
    "        # compute loss. transpose puts class dim in correct position for CE loss\n",
    "        loss = cost_fn(output.transpose(1,2), reversed_input)\n",
    "\n",
    "        # backward pass\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_norm)\n",
    "\n",
    "        # optimizer step\n",
    "        optim.step()\n",
    "\n",
    "        # test\n",
    "        if step % test_interval == 0 or step == steps - 1:\n",
    "            accuracies = test(test_data, model)\n",
    "            print(f'  Step {step:4d}: '\n",
    "                  f'({accuracies[0]:.1%}, {accuracies[1]:6.1%})')\n",
    "\n",
    "train(model, 2000, 64, 10, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model is able to produce the correct second half of the reversed sequence, but not the first half.\n",
    "This is as expected: the positions in the second half of the sequence are able to attend to those in the first half, so they have the necessary information to produce the correct output.\n",
    "But due to the masking in the multihead attention layer the positions in the first half cannot attend to those in the second half, and are thus unable to reproduce them as required.\n",
    "\n",
    "Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence:  [11, 0, 78, 93, 19, 53, 28, 18, 39, 19]\n",
      "Output sequence: [11, 11, 11, 11, 19, 19, 93, 78, 0, 11]\n"
     ]
    }
   ],
   "source": [
    "input = torch.randint(n_min, n_max, (10,))\n",
    "output = model.greedy_output(input)\n",
    "\n",
    "print(f'Input sequence:  {input.tolist()}')\n",
    "print(f'Output sequence: {output.tolist()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the second half of the output sequence is precisely the reverse of the first half of the input sequence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
